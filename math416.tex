\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{parskip}

% setup the header
\pagestyle{fancy}
\lhead{Will Song}
\chead{Math 416}
\rhead{\today}

% setup definition/theorem etc
\newtheoremstyle{norm}
{3pt}
{3pt}
{}
{}
{\bf}
{:}
{.5em}
{}

\theoremstyle{norm}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{df}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{st}{Step}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{clm}[thm]{Claim}
\newtheorem{exr}[thm]{Exercise}
\newtheorem{ex}[thm]{Example}
\newtheorem{prb}[thm]{Problem}

% just useful shorthand
\renewcommand{\st}{\,\operatorname{s.t.}\,}
\let\hom\relax
\DeclareMathOperator{\hom}{Hom}

% for easier math
\everymath{\displaystyle}

\title{Math 416 Notes}
\author{Will Song}
\date{Fall 2014}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Definitions}
This section is reserved for defining what certain things are and for
propositions with very short proofs.

\subsection{Groups}
\begin{df}
A \textbf{semi-group} is a set $G$ with an associative binary operation $* : G
\times G \rightarrow G$.
\end{df}

\begin{df}
A semigroup $G$ is a \textbf{monoid} if we can find an identity $1 \in G \, \st
\, \forall g \in g, g * 1 = 1 * g = g$.
\end{df}

\begin{prop}
The identity is unique.
\end{prop}
\begin{proof}
Suppose $1, 1' \in G$ are both identities. Then
\[ 1 = 1 * 1' = 1'. \]
\end{proof}

\begin{df}
A set $G$ with an associative binary operation $* : G \times G
\rightarrow G$ is called a \textbf{group} if it satisfies the following
properties:
\begin{itemize}
\item $\forall g \in G, \exists 1 \in G \st g * 1 = 1 * g = g$
\item $\forall g \in G, \exists g^{-1} \in G \st g * g^{-1} = g^{-1} * g
= 1$
\end{itemize}
\end{df}
\begin{rem}
A group is equivalent to a monoid with inverses.
\end{rem}
\begin{prop}
Inverses are unique.
\end{prop}
\begin{proof}
Let $x_1, x_2$ both be inverses of $g \in G$. Then
\[ x_1 = x_1 * g * x_2 = x_2. \]
\end{proof}

\begin{df}
A group $G$ is called abelian or commutative if and only if $\forall a,
b \in G, a * b = b * a$.
\end{df}

\begin{df}
Let $G, H$ be groups. A \textbf{group homomorphism} is a map $f : G
\rightarrow H$ that preserves multiplication. In other words,
\[ f(ab) = f(a) f(b) \quad \forall a, b \in G. \]
\end{df}

\begin{df}
Given two sets $X, Y$, we define their direct product to be
\[ X \times Y = \left\lbrace \left(x, y\right) : x \in X, y \in Y
\right\rbrace. \]
\end{df}

\begin{df}
Let $\varphi : G \rightarrow H$ be a group homomorphism. Define the
\textbf{kernel} of $\varphi$ to be
\[ \ker \varphi = \left\lbrace g \in G : \varphi(g) = 1 \right\rbrace \]
where $1$ is the identity in $H$.
\end{df}

\begin{df}
Let $G$ be a semigroup/monoid/group. We say $H \leq G$ is a
\textbf{sub-semigroup} if $h_1, h_2 \in H \implies h_1 h_2 \in H$. If
$G$ is monoid and $1 \in H$, then $H$ is a \textbf{sub-monoid}. Finally,
if $H$ is closed under multiplication, $H$ is a \textbf{sub-group}.
\end{df}

\begin{df}
A \textbf{left coset} of a subgroup $H$ of $G$ with respect to an element $g
\in G$ is the set
\[ gH = \left\lbrace gh : h \in H \right\rbrace. \]
A \textbf{right coset} is defined similarly, but with multiplication on
the right.
\end{df}

\begin{df}
An \textbf{equivalence relation} $\sim$ is something that is symmetric,
reflexive, and transitive. For example, $=$ for the set of integers.
\end{df}
\begin{prop}
An equivalence relation $\sim$ partitions a set $X$ into cosets if $X$
is a group or more generally, equivalence classes.
\end{prop}
\begin{proof}
For any subgroup $H$, we define the equivalence relation $\sim$ on $X$
to be $a \sim b$ if $a = bh$ for some $h \in H$. \\
\textbf{Reflexivity:} $a = ah$ for $h = 1 \in H$ because $H$ is a
subgroup. \\
\textbf{Symmetry:} If $a = bh$, then $b = ah^{-1}$, but $H$ is a
subgroup so $h^{-1} \in H$ is true, so $b \sim a$ as well. \\
\textbf{Transitivity:} Suppose $a \sim b$ and $b \sim c$. Then $a =
bh_1$ and $b = ch_2$, so $a = ch_2h_1$, but $h_2 h_1 \in H$ because $H$
is a subgroup. \\
So the equivalence classes of this equivalence relation are precisely
the left cosets of $X$. Whether or not this works for any other
equivalence relation, if there are any, I do not know.
\end{proof}
\begin{rem}
Because of the previous proposition, it makes sense to speak of the
quotient $X/\sim$ (the equivalence classes of $X$ under $\sim$) as well
as define a mapping $\pi : X \rightarrow X/\sim$ that takes any element
$x \in X$ to its coresponding equivalence class.
\end{rem}

\begin{df}
Call a subgroup $H \leq G$ normal if $\forall h \in H, g \in G$ we have
\[ ghg^{-1} \in H. \]
\end{df}
\begin{rem}
Any abelian subgroup is normal.
\end{rem}
\begin{prop}
$\ker \varphi$ is normal for a homomorphism $\varphi$
\end{prop}
\begin{proof}
We begin by showing that $\ker \varphi$ is a subgroup. \\
\textbf{Identity:} $\varphi(a) = \varphi(1a) = \varphi(1) \varphi(a)$ so
$1 \in \ker \varphi$ \\
\textbf{Inverse:} Suppose $a \in \ker \varphi$. We wish to show that
$a^{-1} \in \ker \varphi$, or $\varphi(a^{-1}) = 1$. But this is easy
because $1 = \varphi(1) = \varphi(aa^{-1}) = \varphi(a) \varphi(a^{-1})
= 1 \varphi(a^{-1})$. \\
\textbf{Associativity:} Suppose $a, b, c \in \ker \varphi$. Then,
\[ \varphi(a(bc)) = \varphi(a)(\varphi(b)\varphi(c)) =
(\varphi(a)\varphi(b))\varphi(c) = \varphi((ab)c) \]
for the silly reason that $\varphi(a) = \varphi(b) = \varphi(c) = 1$. \\
Now we can do normality. \\
\textbf{Normality:} Suppose $h \in \ker \varphi$. Then
\[ \varphi(ghg^{-1}) = \varphi(g)\varphi(h)\varphi(g^{-1}) = \varphi(g)
\varphi(g^{-1}) = \varphi(g g^{-1}) = \varphi(1) = 1, \]
so $ghg^{-1} \in \ker \varphi$ for all $g \in G$.
\end{proof}

\subsection{Rings}
\begin{df}
A \textbf{ring} is a set $R$ combined with two binary operations $+$ and
$*$ such that,
\begin{itemize}
\item $R$ is an abelian group under $+$ with identity $0$. The inverse
of $r \in R$ is usually denoted as $-r$.
\item $R$ is a monoid under $*$ with identity $1$
\item $*$ is distributive over $+$
\end{itemize}
\end{df}

\begin{df}
A ring is called commutative if multiplication in the ring is
commutative.
\end{df}

\begin{prop}
Let $R$ be a ring. Then $0 * r = 0 = 0 * r \quad \forall r \in R$.
\end{prop}
\begin{proof}
We compute
\[ 0 * r = \left(0 + 0\right) * r = 0 * r + 0 * r. \]
We add the inverse to get $0 * r = 0$. The right hand equality is done
in a similar fashion.
\end{proof}

\begin{prop}
Let $R$ be a ring. Then $\left(-1\right) * r = -r = r * \left(-1\right)$.
\end{prop}
\begin{proof}
Again, compute
\[ \left(-1\right) * r + r = \left(-1\right) * r + 1 * r = \left(-1 +
1\right) * r = 0 * r = 0. \]
We get the desired result by adding the inverse of $r$. The same can be
done with the right hand equality.
\end{proof}

\begin{df}
A commutative ring $R$ is called a \textbf{field} if $R \backslash \lbrace 0
\rbrace $ is a group with respect to multiplication.
\end{df}

\begin{df}
We can also have a \textbf{ring homomorphism} $\varphi : R \rightarrow
S$ which preserves both addition and multiplication. Most importantly,
we should also have $\varphi(1) = 1$.
\end{df}

\subsection{Modules}
\begin{df}
A \textbf{left $R$-module} is an additive group $M$ (meaning $M$ is an
abelian group with operation $+$) with an additional left multiplication
by elements in $R$, which shall be called \textbf{scalar
multiplication}, denoted here by $\cdot : R \times M \rightarrow M$,
such that
\begin{itemize}
\item $r_1 \cdot \left(r_2 \cdot m\right) = \left(r_1 * r_2\right) \cdot
m \quad \forall r_1, r_2 \in R, m \in M$
\item $\left(r_1 + r_2\right) \cdot m = r_1 \cdot m + r_2 \cdot m \quad
\forall r_1, r_2 \in R, m \in M$ and $r \cdot \left(m_1 + m_2\right) = r
\cdot m_1 + r \cdot m_2 \quad \forall r \in R, m_1, m_2 \in M$.
\item $1 \cdot m = m \quad \forall m \in M$.
\end{itemize}
\end{df}

\begin{df}
A \textbf{right $R$-module} is defined similarly, except $R$ acts on $M$
on the right.
\end{df}

\begin{df}
If $M$ is a left $R$-module and a right $S$-module, then we call it an
\textbf{$R$-$S$-bimodule} if it satisfies
\[ r\left(ms\right) = \left(rm\right)s. \]
\end{df}
\begin{rem}
An $R$-$R$-bimodule is also called an $R$-bimodule.
\end{rem}

\begin{rem}
For a left $R$-module $M$, we can define an analogous right $R$-module
$M$ by assigning $m \cdot r = rm$. Notice that this gives
\[ \begin{aligned}
\left(r_1 r_2\right) m &= r_1 \left(r_2 m\right) \\
&= \left(m r_2\right) r_1 \\
&= m \left(r_2 r_1\right) \\
&\neq m \left(r_1 r_2\right)
\end{aligned} \]
in general. The first and last expressions are equal if and only if $R$
is a commutative ring. If this is the case, we just say that $M$ is an
$R$-module.
\end{rem}
\begin{rem}
Notice that $R$ is also a $R$ module, where scalar multiplication is
just multiplication in $R$.
\end{rem}

\begin{cor}
All abelian groups are $\mathbb{Z}$-modules.
\end{cor}

\begin{df}
We can also have an \textbf{$R$-module homomorphism} $\varphi : M
\rightarrow N$ that preserves addition in $M$ and the action of $R$.
That is, $\varphi(m_1 + m_2) = \varphi(m_1) + \varphi(m_2)$ and
$\varphi(rm) = r \varphi(m)$.
\end{df}

\begin{df}
Let $M_1, M_2$ be $R$-modules. Define the direct sum of $M_1$ and $M_2$,
denoted $M_1 \oplus M_2$, to be the module whose underlying group is
$M_1 \times M_2$. The action of $R$ on $M_1 \times M_2$ is done
component-wise. \\
We can also generalize this to
\[ R \oplus R \oplus \cdots \oplus R = R^{\oplus n} \]
where there are $n$ $R$s on the left hand side.
\end{df}

\section{More Modules}
This section is reserved for some crucial propositions regarding
modules.

\subsection{Homomorphisms With Modules}

\begin{df}
We will use the notation $\hom(X, Y)$ to denote the set of all
homomorphisms from $X$ to $Y$.
\end{df}

\begin{prop}
Let $\varphi$ be a homomorphism. Then $\varphi$ is injective if and only
if $\ker \varphi = \lbrace 0 \rbrace$.
\end{prop}
\begin{proof}
We first prove that $\varphi$ is injective if $\ker \varphi = \lbrace 0
\rbrace$. Suppose $\varphi(\alpha) = \varphi(\beta)$. Then
$\varphi(\alpha) + (-\varphi(\beta)) = 0$, or $\varphi(\alpha) +
\varphi(-\beta) = 0 \iff \varphi(\alpha + (-\beta)) = 0$. However, $\ker
\varphi = \lbrace 0 \rbrace$ so $\alpha + (-\beta) = 0$, or $\alpha =
\beta$. \\
Now for the other direction. Suppose $\varphi$ is injective. We have
that $\varphi(\alpha) = \varphi(0 + \alpha) = \varphi(0) +
\varphi(\alpha) \implies \varphi(0) = 0$, so $\varphi(\alpha) = 0 =
\varphi(0) \implies \alpha = 0$ by injectivity, so we are done.
\end{proof}

\begin{prop}
Take a $R$-module $M$. The map $\chi : \hom_R(R, M) \rightarrow M$ by
$\chi(\varphi) = \varphi(1)$, or $\varphi \mapsto \varphi(1)$ is an
\textbf{isomorphism}, or a homomorphism that admits an inverse. In other
words, a homomorphism from a ring $R$ to a $R$-module is determined by
its value at $1$.
\end{prop}
\begin{proof}
Let $\varphi, \psi \in \hom_R(R, M)$ and $r \in R$. We can compute that
\[ \chi(\varphi + r \psi) = (\varphi + r \psi)(1) = \varphi(1) + r
\psi(1) = \chi(\varphi) + r \chi(\psi), \]
which gives that $\chi$ is a $R$-module homomorphism. \\
Now suppose $\varphi = \ker \chi$. Then $\forall r \in R$, 
\[ \varphi(r) = r \varphi(1) = r \chi(\varphi) = r * 0 = 0, \]
implying $\varphi$ is injective. \\
Now suppose $m \in M$. We can define a $\psi_m : R \rightarrow M$ by
$\psi_m(r) = rm$. We can easily compute that
\[ \psi_m(r_1 + k r_2) = = r_1 m + (k r_2) m = r_1 m + k (r_2 m) =
\psi_m(r_1) + k \psi_m(r_2), \]
so $\psi_m$ is a $R$-module homomorphism, meaning $\psi_m \in \hom_R(R,
M)$. Moreover, $\chi(\psi_m) = \psi_m(1) = 1m = m$ so $\chi$ is
surjective. \\
Because $\chi$ is a homomorphism and is bijective, it is an isomorphism
and we are done.
\end{proof}

\begin{prop}
We also have the following bijections:
\[ \hom_R(X_1 \cup X_2, Y) \leftrightarrow \hom_R(X_1, Y) \times
\hom_R(X_2, Y) \]
and
\[ \hom_R(X, Y_1 \times Y_2) \leftrightarrow \hom_R(X, Y_1) \times
\hom_R(X, Y_2) \]
\end{prop}
\begin{proof}
I will include an example, taken from Evan Chen. Suppose you want to
define a function on the set $\lbrace 1, 2, 3 \rbrace$. It is the same
as defining a function on the set $\lbrace 1, 2 \rbrace$ and then
another function on the set $\lbrace 3 \rbrace$. \\
Basically, you take two projections $\alpha_1, \alpha_2$ into $X_1
\sqcup X_2$ and the compose them with your function from $X_1 \sqcup
X_2$ to $Y$. \\
Seeing the logic behind the 2nd is left as an exercise to the reader.
\end{proof}

\begin{prop}
We can extend this to direct sums as well. Take any two $R$-modules
$M_1, M_2$. Then there exists a bijection
\[ \hom_R(M_1 \oplus M_2, Y) \leftrightarrow \hom_R(M_1, Y) \times
\hom_R(M_2, Y) \]
and
\[ \hom_R(X, M_1 \oplus M_2) \leftrightarrow \hom_R(X, M_1) \times
\hom_R(X, M_2) \]
\end{prop}

\begin{rem}
We can now consider larger direct sums $M_1, M_2, \dots, M_n$. However,
we are more interested in the bijection
\[ \hom_R(R^{\oplus n}, M) \leftrightarrow \hom_R(R, M)^n \simeq M^n \]
\end{rem}

\begin{prop}
Let $A$ be an indexing set, and consider the set of modules $(M_a)_{a
\in A}$. Then
\[ M = \prod_{a \in A} M_a \]
also has a module structure.
\end{prop}

\begin{df}
We define the infinite direct sum $\bigoplus_a M_a$, which is a subset
of $\prod_a M_a$, where all but finitely many indices $A$ is zero.
\end{df}

\section{Vector Spaces}
We should now have enough abstract definitions to introduce vector
spaces in an abstract enough way. I am purposefully trying to stay away
from commutative maps because they confuse me and learning linear
algebra at such an abstract level is confusing enough for me.

\begin{df}
A vector space $V$ over a field $F$ is an abelian group under $+$ and an
$F$-module.
\end{df}

\begin{df}
The span of a set of vectors $v_1, v_2, \dots, v_k$ is defined as the
set
\[ \left\lbrace \sum_i a_i v_i : a_i \in F \right\rbrace. \]
Similarly, a set of vectors span a set of every element of that set can
be written as a linear combination of the set of vectors.
\end{df}

\begin{df}
A set of vectors $v_1, \dots, v_k$ are said to be linearly independent
if $\sum_i a_i v_i = 0 \Leftrightarrow a_i = 0 \quad \forall i$.
\end{df}

\begin{rem}
Notice that this guarantees that each decompisition into the linearly
independent vectors is unique.
\end{rem}

We now prove a fundamental lemma regarding linearly independent,
spanning sets.
\begin{lem}[Steinitz Exchange Lemma]
Let $V$ be a vector space. Let $v_1, v_2, \dots, v_k$ be a set of
linearly independent vectors in $V$. Let $w_1, w_2, \dots, w_m$ span
$V$. We then have $k \leq m$ and (possibly after reordering $w_i$), the
set of vectors $v_1, \dots, v_k, w_{k + 1}, \dots, w_m$ also span $V$.
\end{lem}

\begin{proof}
We induct on $k$ to get our result. The case $k = 0$ is obvious, as we
don't need to replace anything and the statement of the lemma solves
itself. So suppose the lemma holds positive for some $k < m$. Then,
possibly after reordering, the vectors $v_1, v_2, \dots, v_k, w_{k + 1},
\dots, w_m$ span $V$. We can then write
\[ v_{k + 1} = \sum_i \alpha_i v_i + \sum_i \alpha_i w_i. \]
At least one $\alpha_j$ must be non-zero, so we immediately get $k < m$,
or $k + 1 \leq m$. We can now ``solve'' for $w_{k + 1}$ to get
\[ w_{k + 1} = \frac{1}{\alpha_{k + 1}} \left(v_{k + 1} - \sum_{i =
1}^{k} \alpha_i v_i - \sum_{i = k + 2}^{m} \alpha_i w_i\right). \]
Hence $w_{k + 1}$ is in the span of the set $\lbrace v_1, \dots, v_{k +
1}, w_{k + 2}, \dots, w_m \rbrace$, which means that everything in the
span of $\lbrace v_1, \dots, v_k, w_{k + 1}, \dots, w_m \rbrace$ is also
in the span of $\lbrace v_1, \dots, v_{k + 1}, w_{k + 2}, w_m \rbrace$,
finishing the proof.
\end{proof}

\begin{df}
The cardinality of a set of linearly independent vectors that span a
vector space $V$ is called the dimension of $V$, denoted $\dim(V)$.

This is also the cardinality of the largest set of linearly independent
vectors in $V$. This is easily seen by the above lemma.
\end{df}

\begin{df}
We say $W$ is a subspace of $V$ over $K$ if and only if $W$ satisfies:
\begin{itemize}
\item $0 \in W$
\item $w_1 + w_2 \in W \quad \forall w_1, w_2 \in W$
\item $k w \in W \quad \forall w \in W, k \in F$.
\end{itemize}
\end{df}

\end{document}
