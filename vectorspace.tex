\section{Vector Spaces and Related Things}
\subsection{Vector Spaces}
We should now have enough abstract definitions to introduce vector
spaces in an abstract enough way. I am purposefully trying to stay away
from commutative maps because they confuse me and learning linear
algebra at such an abstract level is confusing enough for me.

\begin{df}
A \textbf{vector space} $V$ over a field $F$ is an abelian group under
$+$ and an $F$-module.
\end{df}

\begin{df}
The \textbf{span} of a set of vectors $v_1, v_2, \dots, v_k$ is defined
as the set
\[ \left\lbrace \sum_i a_i v_i : a_i \in F \right\rbrace. \]
Similarly, a set of vectors span a set of every element of that set can
be written as a linear combination of the set of vectors.
\end{df}

\begin{df}
A set of vectors $v_1, \dots, v_k$ are said to be \textbf{linearly
independent} if $\sum_i a_i v_i = 0 \Leftrightarrow a_i = 0 \quad
\forall i$.
\end{df}

\begin{rem}
Notice that this guarantees that each decompisition into the linearly
independent vectors is unique.
\end{rem}

We now prove a fundamental lemma regarding linearly independent,
spanning sets.
\begin{lem}[Steinitz Exchange Lemma]
Let $V$ be a vector space. Let $v_1, v_2, \dots, v_k$ be a set of
linearly independent vectors in $V$. Let $w_1, w_2, \dots, w_m$ span
$V$. We then have $k \leq m$ and (possibly after reordering $w_i$), the
set of vectors $v_1, \dots, v_k, w_{k + 1}, \dots, w_m$ also span $V$.
\end{lem}

\begin{proof}
We induct on $k$ to get our result. The case $k = 0$ is obvious, as we
don't need to replace anything and the statement of the lemma solves
itself. So suppose the lemma holds positive for some $k < m$. Then,
possibly after reordering, the vectors $v_1, v_2, \dots, v_k, w_{k + 1},
\dots, w_m$ span $V$. We can then write
\[ v_{k + 1} = \sum_i \alpha_i v_i + \sum_i \alpha_i w_i. \]
At least one $\alpha_j$ must be non-zero, so we immediately get $k < m$,
or $k + 1 \leq m$. We can now ``solve'' for $w_{k + 1}$ to get
\[ w_{k + 1} = \frac{1}{\alpha_{k + 1}} \left(v_{k + 1} - \sum_{i =
1}^{k} \alpha_i v_i - \sum_{i = k + 2}^{m} \alpha_i w_i\right). \]
Hence $w_{k + 1}$ is in the span of the set $\lbrace v_1, \dots, v_{k +
1}, w_{k + 2}, \dots, w_m \rbrace$, which means that everything in the
span of $\lbrace v_1, \dots, v_k, w_{k + 1}, \dots, w_m \rbrace$ is also
in the span of $\lbrace v_1, \dots, v_{k + 1}, w_{k + 2}, w_m \rbrace$,
finishing the proof.
\end{proof}

\begin{df}
The cardinality of a set of linearly independent vectors that span a
vector space $V$ is called the \textbf{dimension} of $V$, denoted
$\dim(V)$.

This is also the cardinality of the largest set of linearly independent
vectors in $V$. This is easily seen by the above lemma.
\end{df}

\begin{df}
We say a set of vectors is a \textbf{basis} for a vector space $V$ if
the set contains linearly independent vectors and spans $V$.
\end{df}

\begin{df}
We say $W$ is a \textbf{subspace} of $V$ over $F$ if and only if $W$
satisfies:
\begin{itemize}
\item $0 \in W$
\item $w_1 + w_2 \in W \quad \forall w_1, w_2 \in W$
\item $k w \in W \quad \forall w \in W, k \in F$.
\end{itemize}
\end{df}

Anyways, here are some examples of vector spaces and subspaces
\begin{itemize}
\item Any field $F$, such as $\mathbb{R}$ or $\mathbb{C}$
\item Any repeated direct sum of a field $F$, such as $\mathbb{R}^3$
\item The space of functions on $\mathbb{R}$
\begin{itemize}
\item Odd functions on $\mathbb{R}$
\item Even functions on $\mathbb{R}$
\end{itemize}
\end{itemize}

\begin{prop}
Let $V, W$ be subspaces. Then
\[ \dim(V) + \dim(W) = \dim(V + W) + \dim(V \cap W) \]
where $V + W = \lbrace v + w : v \in V, w \in W \rbrace$.
\end{prop}

\begin{proof}
Let $\alpha_1, \dots, \alpha_n$ be a basis for $V \cap W$. We can then
find the basis $\alpha_1, \dots, \alpha_n, v_1, \dots, v_k$ for $V$ and
the same for $W$, $w_1, \dots, w_l, \alpha_1, \dots, \alpha_n$. If we
can prove that $\alpha_1, \dots, \alpha_n, v_1, \dots, v_k, w_1, \dots,
w_l$ are a basis for $V + W$, we are done because
\[ (n + k) + (n + l) = (n + k + l) + n \]
It is obvious that they span $V + W$ by the definition of span, so it
suffices to show that they are linearly independent. Suppose that
\[ 0 = \sum a_i \alpha_i + \sum b_i v_i + \sum c_i w_i \]
We rearrange this equation to arrive at
\[ -\sum a_i \alpha_i = \sum b_i v_i + \sum c_i w_i, \]
implying $-\sum a_i \alpha_i, \sum b_i v_i + \sum c_i w_i \in V \cap W$.
However, $v_i$ and $w_i$ are not in the set $V \cap W$, otherwise they
would not be linearly independent with the $\alpha_i$, which implies
that $b_i = c_i = 0$ for all $i$, otherwise the RHS would not be in the
set $V \cap W$. Now use the fact that $\alpha_i$ form a basis to get
that $a_i = 0$, so we are done.
\end{proof}

\begin{prop}
A vector space $V$ over a field $F$ with dimension $d$ is isomorphic to
$F^d$.
\end{prop}

\begin{proof}
We just choose a basis and express each vector in $V$ as the
coefficients in the expansion as a sum of linearly independent vectors.
\end{proof}

For example, consider the space of polynomials with complex coefficients
with degree less than or equal to $3$. We will call this space
$\mathcal{P}_3$. Then the vector $1 + (3 - 2i)x + 7x^2$ is the same as
$(1, 3 - 2i, 7, 0)$ with the basis of $\lbrace 1, x, x^2, x^3 \rbrace$.

\begin{prb}
Determine a necessary and sufficient condition for a polynomial $P \in
\mathbb{R}[X]$ such that $P(x) \in \mathbb{Z} \forall x \in
\mathbb{Z}$.
% TODO: Actually prove this thing
\end{prb}

\begin{proof}[Solution]
I claim that $P$ must be an integral linear combination of the
polynomials $\binom{X}{k} \forall k \in \mathbb{N}$.
\end{proof}

\subsection{Linear Maps}
\begin{df}
Let $V, W$ be vector spaces.  A map $L : V \rightarrow W$ is said to be
\textbf{linear} if it satisfies:
\begin{itemize}
\item $L(v_1 + v_2) = L(v_1) + L(v_2) \quad \forall v_1, v_2 \in V$
\item $L(c v) = c L(v) \quad \forall c \in F, v \in V$
\end{itemize}
\end{df}

\begin{df}
The \textbf{nullspace} of a linear map $L$ is the same as the kernel of
$L$. We will denote the nullspace with $\mathcal{N}(L)$.
\end{df}

\begin{df}
The \textbf{range} of $L$ is simply the set $\mathcal{R}(L) = \lbrace
L(v) : v \in V \rbrace$.
\end{df}

\begin{prop}
Let $V, W$ be vector spaces and let $L : V \rightarrow W$ be linear.
Then $L$ is injective if and only if $\mathcal{N}(L) = \lbrace 0
\rbrace$.
\end{prop}

\begin{proof}
The forward direction is trivial because $L(v) = 0 = L(0) \implies v =
0$.

Going backwards, suppose $L(v_1) = L(v_2)$. Then, we have
\[ \begin{aligned}
L(v_1) - L(v_2) &= 0 \\
L(v_1 - v_2) &= 0 \\
v_1 - v_2 &= 0 \\
v_1 &= v_2
\end{aligned} \]
by linearity in $L$ and the fact that $v_1 - v_2 \in \mathcal{N}(L)$.
\end{proof}

\begin{thm}[Rank-Nullity Theorem]
Let $V, W$ be finite dimensional vector spaces and let $L : V
\rightarrow W$ be linear. Then
\[ \dim(\mathcal{N}(L)) + \dim(\mathcal{R}(L)) = \dim(V). \]
\end{thm}

\begin{proof}
Let $\lbrace \alpha_i \rbrace_{i = 1}^{k}$ be a basis for
$\mathcal{N}(L)$. By the exchange lemma, we also have $v_{k + 1}, \dots,
v_n$ so that $\alpha_i, v_i$ form a basis for $V$. If we can show that
$\dim(\mathcal{R}(L)) = n - k$, we are done, so we claim that $L(v_{k +
1}), \dots, L(v_n)$ form a basis for $\mathcal{R}(L)$.

\textbf{Span}: Suppose $z \in \mathcal{R}(L)$. Then we have
\[ \begin{aligned}
z &= L(v) \\
z &= L\left(\sum a_i \alpha_i + \sum b_i v_i\right) \\
z &= \sum a_i L(\alpha_i) + \sum b_i L(v_i) \\
z &= \sum b_i L(v_i) \implies z \in \operatorname{span}(L(v_i))
\end{aligned} \]
by linearity in $L$ and the stupid reason that $L(\alpha_i) = 0$.

\textbf{Linear independence}: Suppose $0 = \sum a_i L(v_i)$. Then
$L\left(\sum a_i v_i\right) = 0 \implies \sum a_i v_i \in \mathcal{N}(L)
\implies \sum a_i v_i = \sum b_i \alpha_i$, or $\sum a_i v_i + \sum -b_i
v_i = 0$, giving $a_i, b_i = 0$.
\end{proof}

\begin{rem}
$\dim(\mathcal{N}(L))$ is usually called the nullity of $L$ while
$\dim(\mathcal{R}(L))$ is usually called the rank of $L$, hence the name
of the theorem.
\end{rem}

\subsection{Matrices of Linear Maps}
Because linear maps are linear, they are given completely by what they
do on a basis. This motivates us to find some sort of compact
representation for a linear map.

\begin{df}
The \textbf{matrix} of a linear map $L : V \rightarrow W$ is defined as
follows. We choose a basis for $V$ and $W$, and the columns of the
matrix of $L$ is the coordinate decomposition of $L$ on the basis of
$V$.
\end{df}

\begin{ex}
The matrix of the linear transforomation in $\mathbb{R}^3$ to
$\mathbb{R}^3$ that takes $(1, 0, 0), (0, 1, 0), (0, 0, 1)$ to $(1, 2,
3),  (4, 5, 6), (7, 8, 9)$ would be represented as
\[ \begin{pmatrix}
1 & 4 & 7 \\
2 & 5 & 8 \\
3 & 6 & 9
\end{pmatrix} \]
\end{ex}

\begin{ex}
Consider the polynomial space $\mathcal{P}_3$ and let the linear
transformation be differentiation. The matrix of differentiation (taking
$\mathcal{P}_3$ to $\mathcal{P}_2$) with respect to $1, x, x^2, x^3$ and
$1, x, x^2$ would be
\[ \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3
\end{pmatrix} \]
\end{ex}

We can then easily compute the transformation of any vector once bases
are chosen. This is done by "dotting", or multiplying each row vector's
components with the components of the column vector representation of
the vector being transformed, and then summing up the multiplied
components.

\begin{ex}
\[ \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3
\end{pmatrix} \begin{pmatrix}
1 \\ 2 \\ 3 \\ 4
\end{pmatrix} = \begin{pmatrix}
2 \\ 6 \\ 12
\end{pmatrix}, \]
which is the same as saying $\frac{d}{dx}(1 + 2x + 3x^2 + 4x^3) = 2 + 6x
+ 12x^2$.
\end{ex}

\begin{rem}
We can compose linear transformations. If $T_1 : U \rightarrow V, T_2 :
V \rightarrow W$, then we can compute $T_3 : U \rightarrow W$ with $T_3
= T_2 T_1$, which is defined as $T_3(u) = T_2(T_1(u))$. Computing the
matrix of the resulting transform is also simple, and this is what
defines \textbf{matrix multiplication}. You basically transform each
column vector in the matrix of $T_1$ by $T_2$ and set that as the column
vector in the respective column of $T_3$. If this does not make sense,
experiment a little bit and see why this should be true.
\end{rem}

\begin{rem}
Matrix multiplication is associative. The easiest way to see this is by
going back to composition of transformations. $T_3 (T_2 T_1) = (T_3 T_2)
T_1$ simply because $T_3 (T_2 T_1) u = T_3(T_2(T_1(u))) = (T_3 T_2) T_1
u$.
\end{rem}

\begin{rem}
$A_{ij}$ will denote the $i,j$-th entry of a matrix $A$, or the entry in
the $i$-th row and $j$-th column.
\end{rem}
