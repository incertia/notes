\section{Square Matrices}
Square matrices are very special, so they get their own section.

\subsection{More Matrices}
\begin{df}
We say a matrix $A$ is \textbf{invertible} if there exists another
matrix $A^{-1}$ such that $A A^{-1} = A^{-1} A = I$ where $I$ represents
the matrix of the trivial transformation. Note that this restricts $A$
to square matrices.
\end{df}

\begin{rem}
$(AB)^{-1} = B^{-1} A^{-1}$.
\end{rem}

\begin{prop}
$AB = I$ only implies $BA = I$ in the finite dimensional case.
\end{prop}

\begin{proof}
We have
\[ B = IB = BI = B(AB) = (BA)B, \]
or
\[ (I - BA)B = 0 \implies I - BA = 0 \implies BA = I. \]
\end{proof}

\begin{rem}
Suppose we want to solve the system of equations
\[ \left\lbrace\begin{aligned}
c_{11} x_1 + \dots + c_{1n} x_n &= b_1 \\
c_{21} x_1 + \dots + c_{2n} x_n &= b_2 \\
& \vdots \\
c_{n1} x_1 + \dots + c_{nn} x_n &= b_n \\
\end{aligned} \right. \]
We can now rewrite this entire system as a linear transformation.
\[ \begin{pmatrix}
c_{11} & c_{12} & \cdots & c_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
c_{n1} & c_{n2} & \cdots & c_{nn}
\end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\ x_n
\end{pmatrix} = \begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix} \]
and reduce the problem to finding the inverse of the big matrix on the
left.
\end{rem}

\begin{df}
There are three \textbf{elementary row operations}.
\begin{enumerate}
\item You swap rows $r_i$ and $r_j$
\item Row $r_i \mapsto \lambda r_i$ with $\lambda \neq 0$
\item Row $r_i \mapsto r_i + \lambda r_j$ some $i \neq j$
\end{enumerate}
Each of these operations has their own matrix to represent that
operation, and their inverses also exist.
\end{df}

\begin{prop}
Every invertible matrix $A$ can be transformed into its respective
identity matrix with elementary row operations, or
\[ I = E_n \cdots E_1 A, \]
where $E_i$ denotes some elementary row operation. This is also
equivalent to every invertible matrix is the product of elementary
matrices.
\end{prop}

\begin{df}
A matrix $D$ is said to be \textbf{diagonal} if $D_{ij} = 0$ for all $i
\neq j$.  Computing the products of diagonal matrices is easy because
you end up just multiplying the components. A matrix $A$ is
\textbf{diagonalizable} if there is a basis for which $A$ becomes
diagonal (i.e. a basis consisting of eigenvectors).
\end{df}

\begin{prop}
If $A$ is diagonalizable, $A = PDP^{-1}$ for diagonal $D$ and some
matrix $P$.
\end{prop}

\begin{proof}
This is equivalent to $AP = PD = P \begin{pmatrix} \lambda_1 & & \\ &
\lambda_2 & & \\ & & \ddots & \\ & & & \lambda_n \end{pmatrix} =
\begin{pmatrix} | & | & & | \\ \lambda_1 \mathcal{P}_1 & \lambda_2
\mathcal{P}_2 & \cdots & \lambda_n \mathcal{P}_n \\ | & | & & |
\end{pmatrix}$, where $\mathcal{P}_i$ represents the $i$-th column of
$P$. However, if we look at the LHS, we have $A \mathcal{P}_i =
\lambda_i \mathcal{P}_i$, so $P$'s columns are precisely eigenvectors
for its corresponding eigenvalue.
\end{proof}

\begin{prb}
Matrices behave very much like modules. In fact, $\mathcal{M}(n, n)
\cong F^{n^2}$ as a vector space. Find a matrix representation of $M
\oplus N$ with $M, N$ two arbitrary square matrix spaces.
\end{prb}

\begin{proof}[Solution]
We basically construct a block diagonal matrix as follows
\[ \begin{pmatrix} A_1 & 0 \\ 0 & A_2 \end{pmatrix} A_1 \in M, A_2 \in
N, \]
but actually expanding out the matrices and zeroes in full. This gives
us an injection $\mathcal{M}(m, m) \oplus \mathcal{M}(n, n)
\rightarrowtail \mathcal{M}(m + n, m + n)$ by preserving addition,
scalar multiplication, as well as matrix multiplication.
\end{proof}

\subsection{Linear Things}
It wouldn't be linear algebra without linear things.
\begin{df}
The identity transformation has a corresponding identity matrix $I_n$
where the $ii$-th entries are $1$ and the other entries are $0$.
\end{df}

\begin{df}
We define the \textbf{trace} of a square matrix $A$, denoted $\Tr(A)$,
to be the sum of the elements along the main diagonal of $M$, or,
equivalently, $\sum A_{ii}$.
\end{df}

\begin{prop}
The trace of a matrix is the unique linear transformation $T :
\mathcal{M}(n, n) \rightarrow F$ satisfying:
\begin{itemize}
\item $T(I_n) = n$
\item $T(AB) = T(BA)$
\end{itemize}
\end{prop}

\begin{proof}
Suppose $f$ is another mapping that satisfies the above properties.
Define the matrix $\mathcal{E}_{ij}$ to be the matrix with a $1$ in the
$ij$ slot and a $0$ elsewhere. Here we show that $f(\mathcal{E}_{ij})$
takes the value $0$ whenever $i \neq j$. Indeed, we have
\[ f(\mathcal{E}_{ij}) = f(\mathcal{E}_{i1} \mathcal{E}_{1j}) =
f(\mathcal{E}_{1j} \mathcal{E}_{i1}) = f(0) = 0. \]
On the other hand,
\[ f(\mathcal{E}_{ii}) = f(\mathcal{E}_{i1} \mathcal{E}_{1i}) =
f(\mathcal{E}_{1i} \mathcal{E}_{i1}) = f(\mathcal{E}_{11}). \]
By linearity, $f(A) = \sum A_{ij} f(\mathcal{E}_{ij}) = \sum A_{ii}
f(\mathcal{E}_{11}) = f(\mathcal{E}_{11}) \sum A_{ii} = c \Tr(A)$. Now
we use the value of $f$ on $I$ to get that $c = 1$, and we are done.
\end{proof}

\subsection{MULTIlinear???!}
Yeah multilinearity is a thing.

\begin{df}
A function $f : V_1 \times \cdots \times V_n \rightarrow W$ is
$n$-linear if $f$ is linear in each argument, or $F(v_k) = f(c_1, c_2,
\dots, c_{k - 1}, v_k, c_{k + 1}, \dots, c_n)$ is linear for each $k$.
\end{df}

\begin{df}
A $n$-linear function $f : V_1 \times \cdots \times V_n \rightarrow W$
is said to be \textbf{alternating} if it vanishes when two arguments are
the same.  Note that this can only really happen when at least two of
the vector spaces $V_i$ are the same.
\end{df}

\begin{prop}
Swapping two arguments (given they both belong to the same vector space)
will change the sign of an alternating function.
\end{prop}

\begin{proof}
It suffices to show that $f(a, b) = -f(b, a)$, or $f(a, b) + f(b, a) =
0$, but this is trivial. Why it reduces to a two variable function is
left as an exercise to the reader.
\end{proof}

\begin{rem}
If $f$ is not necessarily linear, we require both $f(a, a) = 0$ and
$f(a, b) + f(b, a) = 0$.
\end{rem}

\begin{thm}[Determinant Thing]
There exists a unique $n$-linear, alternating function $\det : V \times
\cdots \times V \rightarrow F$ isomorphic to $\det : \mathcal{M}(n, n)
\rightarrow F$ satisfying $\det(I) = 1$. This function is given by
$\det(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod A_{i \sigma(i)}$.
Basically what this is saying is that $\det$ should be $n$-linear in the
rows and columns of an $n \times n$ matrix.
\end{thm}

\begin{proof}
This nasty expansion for the determinant is credited to Leibniz. In this
proof, columns and rows can be freely interchanged to get the same
result.

\textbf{Existence}: We show that the Leibniz formula is $n$-linear and
alternating, as it obviously takes on $1$ at $I$. Let $r_i$ denote the
$i$-th row of a matrix $A$ and $r_i^j$ denote the $j$-th entry in the
$i$-th row. Then
\[ \begin{aligned}
\det(r_1, \dots, c r_j, \dots, r_n) &= \sum_{\sigma \in S_n}
\sgn(\sigma) cr_j^{\sigma(j)} \prod_{i = 1, i \neq j}^n r_i^{\sigma(i)}
\\
&= c \sum_{\sigma \in S_n} \sgn(\sigma) \prod r_i^{\sigma(i)} \\
&= c\det(r_1, \dots, r_n), \\
\det(r_1, \dots, v + r_j, \dots, r_n) &= \sum_{\sigma \in S_n}
\sgn(\sigma) \left(v^{\sigma(j)} + r_j^{\sigma(j)}\right) \prod_{i = 1,
i \neq j}^n r_i^{\sigma(i)} \\
&= \sum_{\sigma \in S_n} \sgn(\sigma) v^{\sigma(j)} \prod_{i = 1, i \neq
j}^n r_i^{\sigma(i)} + \sum_{\sigma \in S_n} \sgn(\sigma) \prod
r_i^{\sigma(i)} \\
&= \det(r_1, \dots, v, \dots, r_n) + \det(r_1, \dots, r_n),
\end{aligned} \]
so $\det$ is $n$-linear. Additionally, if $\tau_j^k$ is the
transposition that swaps the $j$-th and $k$-th entries and we let
$\sigma' = \tau_j^k \sigma$, we also have
\[ \begin{aligned}
\det(A) &= \sum_{\sigma \in S_n, \sigma(j) < \sigma(k)} \sgn(\sigma)
r_j^{\sigma(j)} r_k^{\sigma(k)} \prod_{i = 1, i \neq j, k}
r_i^{\sigma(i)} + \sgn(\sigma') r_j^{\sigma'(j)} r_k^{\sigma'(j)}
\prod_{i = 1, i \neq j, k} r_i^{\sigma'(j)} \\
&= \sum_{\sigma \in S_n, \sigma(j) < \sigma(k)} \sgn(\sigma)
\left(\prod_{i = 1, i \neq j, k}^n r_i^{\sigma(i)}\right)
\left(r_j^{\sigma(j)} r_k^{\sigma(k)} - r_j^{\sigma(k)}
r_k^{\sigma(j)}\right).
\end{aligned} \]
Hence if $r_j = r_k$, $\det(A) = 0$ so $\det$ is alternating as well. We
have now shown that the Leibniz formula does indeed give a $n$-linear,
alternating function on the row-space of a matrix.

\textbf{Uniqueness}: Let $f$ be another such function. We can represent
each row $r_i = \sum_k \mathcal{E}_k r_i^k$ (for columns, swap the
order), where $\mathcal{E}_k$ represents the $k$-th row of $I$. By
linearity, we then have
\[ f(A) = f\left(\sum_k \mathcal{E}_k r_1^k, \dots, \sum_k \mathcal{E}_k
r_n^k\right) = \sum_{k_1, \dots, k_n = 1}^n \left(\prod_{i = 1}^n
r_i^{k_i}\right) f(\mathcal{E}_{k_1}, \dots, \mathcal{E}_{k_n}). \]
However, $f$ is alternating, so it only suffices to consider
permutations, so we have
\[ f(A) = \sum_{\sigma \in S_n} \left(\prod_i r_i^{\sigma(i)}\right)
f(\mathcal{E}_{\sigma(1)}, \dots, \mathcal{E}_{\sigma(n)}). \]
We now use alternating again to rearrange the term on the right to
$f(I)$, and we use $\sgn$ to count the number of transpositions needed,
so we ultimately have
\[ f(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_i r_i^{\sigma(i)}
f(I). \]
Now use $f(I) = 1$ to get the desired result.
\end{proof}

\begin{rem}
Because the determinant is unique, you do not have to show that an
alternate form for the determinant takes on that ugly Leibniz formula.
You just have to show that it is $n$-linear, alternating, and $1$ on the
identity.
\end{rem}

\begin{prb}
Verify that the Leibniz formula produces the correct results for the $2
\times 2$ and $3 \times 3$ case.
\end{prb}

\begin{proof}[Solution]
Trivial.
\end{proof}

\begin{rem}
It is easy to calculate the determinant of triangular matrices.
\end{rem}

\begin{prop}
The determinant is completely multiplicative, or
\[ \det(AB) = \det(A)\det(B). \]
\end{prop}

\begin{proof}
%TODO: finish the proof
filler
\end{proof}

\begin{prb}
Define the square matrix $\mathcal{G}_n$ as the square matrix where the
$(i, j)$ entry is $\gcd(i, j)$. Compute $\det(\mathcal{G}_n)$.
\end{prb}

\begin{proof}[Solution]
A common way to compute determinants of complex matrices is to use what
is called the $LU$-decomposition, where we write a matrix $M$ as $M =
LU$ where $L$ is lower triangular and $U$ is upper triangular. So now
define
\[ \begin{aligned}
L_{ij} &= \left\lbrace \begin{aligned}
1 \qquad j \mid i \\
0 \qquad j \nmid i
\end{aligned}\right. \\
U_{ij} &= \left\lbrace \begin{aligned}
\varphi(j) \qquad i \mid j \\
0 \qquad i \nmid j
\end{aligned}\right.
\end{aligned} \]
We can now compute
\[ \begin{aligned}
M_{ij} &= \sum_k L_{ik} U_{kj} \\
&= \sum_{k \mid i, j} \varphi(k) \\
&= \gcd(i, j)
\end{aligned} \]

Then we easily have that the determinant $\det(\mathcal{G}_n) = \det(LU)
= \det(L) \det(U) = \prod_{i = 1}^{n} \varphi(i).$

\begin{rem}
In this proof, we used the fact that $\sum_{d \mid n} \varphi(d) = n$.
The easiest way to see why this is true is to count the number of
fractions $\frac{1}{n}, \frac{2}{n}, \frac{3}{n}, \dots, \frac{n}{n}$
after reducing.
\end{rem}
\end{proof}

\begin{df}
The \textbf{cofactor} $C_{ij}$ of a matrix $A$ is defined as $C_{ij} =
(-1)^{i + j} M_A(i, j)$ where $M_A(i, j)$ is the $ij$ \textbf{minor} of
$A$, found by taking the determinant of the resultant matrix given by
deleting the $i$-th row and $j$-th column of $A$. This gives rise to the
\textbf{cofactor matrix}.
\end{df}

\begin{df}
The \textbf{adjugate} or \textbf{adjunct} of a matrix $A$ is the
transpose of the cofactor matrix, $\adj(A) = C^T$.
\end{df}

\begin{thm}[Cofactor Expansion]
An alternate form for the determinant is given by
\[ \det(A) = \sum A_{ij} C_{ij}, \]
where the sum is taken over \textbf{ONE OF} $i, j$, depending on what
you are expanding along, a row or a column.
\end{thm}

\begin{thm}[Adjunct Determinant]
$\adj(A) A = \det(A) I$.
\end{thm}

\subsection{Eigenstuff}

\begin{df}
$\lambda$ is said to be an \textbf{eigenvalue} for $L : V \rightarrow V$
with corresponding non-zero \textbf{eigenvector} $v$ if and only if $Lv
= \lambda v$.
\end{df}

\begin{df}
We say that the \textbf{eigenspace} of $\lambda$ is the set
\[ E_{\lambda} = \lbrace v \in V : Lv = \lambda v \rbrace. \]
Notice that $0 \in E_{\lambda}$.
\end{df}

\begin{df}
We can extend this notion of eigenvalue to a \textbf{generalized
eigenvalue} $\lambda$ if some non-zero vector $v \in V$ satisfies $(L -
\lambda I)^k (v) = 0$ for some $k$.
\end{df}

\begin{df}
Similarly, we define the \textbf{generalized eigenspace} to be
\[ K_{\lambda} = \lbrace v \in V : \exists k : (L - \lambda I)^k (v) = 0
\rbrace. \]
\end{df}

\begin{prop}
$K_{\lambda}$ is an \textbf{invariant subspace}. That is, $Lv \in
K_{\lambda} \quad \forall v \in K_{\lambda}$.
\end{prop}

\begin{proof}
$K_{\lambda}$ is obviously a subspace, so we show that it is invariant.
Compute
\[ (L - \lambda I)^k (Lv) = L (L - \lambda I)^k (v) = L(0) = 0. \]
The key fact here is that $L$ commutes with itself and $I$.
\end{proof}

\begin{prop}
Suppose $\lambda \neq \mu$. Then $K_{\lambda} \cap K_{\mu} = \lbrace 0
\rbrace$.
\end{prop}

\begin{proof}
Recall that $L : V \rightarrow W$ is injective if and only if
$\mathcal{N}(L) = \lbrace 0 \rbrace$. Take $(L - \mu I) : K_{\lambda}
\rightarrow V$. It is sufficient to show that that is injective
(composition of injective functions is injective, so $(L - \mu I)^j (v)
= 0 \implies v = 0$), so suppose $(L - \lambda I)^k (v) = 0 = (L - \mu
I)(v)$. We get $Lv = \mu v$, so we substitute
\[ 0 = (L - \lambda I)^k (v) = (\mu - \lambda)^k (v) \implies v = 0 \]
because $\mu \neq \lambda$.
\end{proof}

\begin{df}
The \textbf{characteristic polynomial} of a matrix $L$ is the polynomial
$\chi_L = \det(L - \lambda I)$.
\end{df}

\begin{df}
The \textbf{minimal polynomial} $\pi_L$ of a matrix $L$ is the unique
monic polynomial of lowest degree that kills $L$.
\end{df}

\begin{thm}[Cayley-Hamilton]
$\chi_L(L) = 0$.
\end{thm}

\begin{proof}
This should intuitively make sense, as $\chi_L(L) = \det(L - LI) =
\det(0) = 0$, but what this is really saying is we are plugging in $L$
as $\lambda$ in the \emph{polynomial expansion} of $\det(L - \lambda
I)$.

Suppose $L$ is diagonalizeable. Then $L = PDP^{-1}$. We easily compute
that
\[ \chi_L(D) = \begin{pmatrix}
\chi_L(\lambda_1) & & \\
& \chi_L(\lambda_2) & & \\
& & \ddots & \\
& & & \chi_L(\lambda_n)
\end{pmatrix}, \]
but $\chi_L(\lambda_k) = 0$ so $\chi_L(D) = 0$. Now compute
\[ \chi_L(L) = \chi_L(PDP^{-1}) = P\chi_L(D)P^{-1} = 0 \]
to get the result for diagonalizable matrices.

Now we use the fact that every matrix can be represented as a limit of
an infinite sequence of diagonalizable matrices (the limit of a sequence
of matrices is the limit of the entries) to get the desired result.
\end{proof}

\begin{cor}
$\pi_L \mid \chi_L$.
\end{cor}

\begin{prop}
The zeroes of $\chi_L$ are the eigenvalues of $L$.
\end{prop}

\begin{lem}
Let $\lambda_1, \dots, \lambda_k$ be the distinct eigenvalues of $L : V
\rightarrow V$. Then
\[ V = K_{\lambda_1} + \cdots + K_{\lambda_k}. \]
\end{lem}

\begin{proof}
We induct on $k$. In the case $k = 1$, we have by Cayley-Hamilton that
$(L - \lambda_1 I)^n = 0$ where $n = \dim V$, which automatically
implies $V = K_{\lambda_1}$. Now suppose $k - 1$ is true. Consider $L$
as $L : \mathcal{R}((L - \lambda_k I)^j) \rightarrow V$.
\end{proof}

\begin{prb}[Fall 2014 Final Exam]
Compute the determinant of the square matrix whose entries along the
main diagonal are $x$ and $1$ everywhere else.
\end{prb}

\begin{proof}[Solution]
We use small matrices as examples to get some motivation. We will also
use $M$ to denote these matrices, indexed by their size $n$.
\[ \begin{aligned}
\det\begin{pmatrix} x \end{pmatrix} &= x = (x - 1)^0 (x + 0) \\
\det\begin{pmatrix} x & 1 \\ 1 & x \end{pmatrix} &= x^2 - 1 = (x -
1)^1(x + 1) \\
\det\begin{pmatrix} x & 1 & 1 \\ 1 & x & 1 \\ 1 & 1 & x \end{pmatrix} &=
x^3 + 2 - 3x = (x - 1)^2 (x + 2)
\end{aligned} \]
So it seems that $\det(M_n) = (x - 1)^{n - 1} (x + (n - 1))$. I claim
that this is indeed the determinant.

We can easily see that $x - 1$ is a factor in the determinant because it
is an eigenvalue (or zero of the characteristic polynomial).
%TODO: finish solution unlike final
\end{proof}
