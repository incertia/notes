\section{Square Matrices}
Square matrices are very special, so they get their own section.

\subsection{More Matrices}
\begin{df}
We say a matrix $A$ is \textbf{invertible} if there exists another
matrix $A^{-1}$ such that $A A^{-1} = A^{-1} A = I$ where $I$ represents
the matrix of the trivial transformation. Note that this restricts $A$
to square matrices.
\end{df}

\begin{rem}
$(AB)^{-1} = B^{-1} A^{-1}$.
\end{rem}

\begin{prop}
$AB = I$ only implies $BA = I$ in the finite dimensional case.
\end{prop}

\begin{proof}
We have
\[ B = IB = BI = B(AB) = (BA)B, \]
or
\[ (I - BA)B = 0 \implies I - BA = 0 \implies BA = I. \]
\end{proof}

\begin{rem}
Suppose we want to solve the system of equations
\[ \left\lbrace\begin{aligned}
c_{11} x_1 + \dots + c_{1n} x_n &= b_1 \\
c_{21} x_1 + \dots + c_{2n} x_n &= b_2 \\
& \vdots \\
c_{n1} x_1 + \dots + c_{nn} x_n &= b_n \\
\end{aligned} \right. \]
We can now rewrite this entire system as a linear transformation.
\[ \begin{pmatrix}
c_{11} & c_{12} & \cdots & c_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
c_{n1} & c_{n2} & \cdots & c_{nn}
\end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\ x_n
\end{pmatrix} = \begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix} \]
and reduce the problem to finding the inverse of the big matrix on the
left.
\end{rem}

\begin{df}
There are three \textbf{elementary row operations}.
\begin{enumerate}
\item You swap rows $r_i$ and $r_j$
\item Row $r_i \mapsto \lambda r_i$ with $\lambda \neq 0$
\item Row $r_i \mapsto r_i + \lambda r_j$ some $i \neq j$
\end{enumerate}
Each of these operations has their own matrix to represent that
operation, and their inverses also exist.
\end{df}

\begin{prop}
Every invertible matrix $A$ can be transformed into its respective
identity matrix with elementary row operations, or
\[ I = E_n \cdots E_1 A, \]
where $E_i$ denotes some elementary row operation. This is also
equivalent to every invertible matrix is the product of elementary
matrices.
\end{prop}

\subsection{Linear Things}
It wouldn't be linear algebra without linear things.
\begin{df}
The identity transformation has a corresponding identity matrix $I_n$
where the $ii$-th entries are $1$ and the other entries are $0$.
\end{df}

\begin{df}
We define the \textbf{trace} of a square matrix $A$, denoted $\Tr(A)$,
to be the sum of the elements along the main diagonal of $M$, or,
equivalently, $\sum A_{ii}$.
\end{df}

\begin{prop}
The trace of a matrix is the unique linear transformation $T :
\mathcal{M}(n, n) \rightarrow F$ satisfying:
\begin{itemize}
\item $T(I_n) = n$
\item $T(AB) = T(BA)$
\end{itemize}
\end{prop}

\begin{proof}
Suppose $f$ is another mapping that satisfies the above properties.
Define the matrix $\mathcal{E}_{ij}$ to be the matrix with a $1$ in the
$ij$ slot and a $0$ elsewhere. Here we show that $f(\mathcal{E}_{ij})$
takes the value $0$ whenever $i \neq j$. Indeed, we have
\[ f(\mathcal{E}_{ij}) = f(\mathcal{E}_{i1} \mathcal{E}_{1j}) =
f(\mathcal{E}_{1j} \mathcal{E}_{i1}) = f(0) = 0. \]
On the other hand,
\[ f(\mathcal{E}_{ii}) = f(\mathcal{E}_{i1} \mathcal{E}_{1i}) =
f(\mathcal{E}_{1i} \mathcal{E}_{i1}) = f(\mathcal{E}_{11}). \]
By linearity, $f(A) = \sum A_{ij} f(\mathcal{E}_{ij}) = \sum A_{ii}
f(\mathcal{E}_{11}) = f(\mathcal{E}_{11}) \sum A_{ii} = c \Tr(A)$. Now
we use the value of $f$ on $I$ to get that $c = 1$, and we are done.
\end{proof}

\subsection{MULTIlinear???!}
Yeah multilinearity is a thing.

\begin{df}
A function $f : V_1 \times \cdots \times V_n \rightarrow W$ is
$n$-linear if $f$ is linear in each argument, or $F(v_k) = f(c_1, c_2,
\dots, c_{k - 1}, v_k, c_{k + 1}, \dots, c_n)$ is linear for each $k$.
\end{df}

\begin{df}
A $n$-linear function $f : V_1 \times \cdots \times V_n \rightarrow W$
is said to be \textbf{alternating} if it vanishes when two arguments are
the same.  Note that this can only really happen when at least two of
the vector spaces $V_i$ are the same.
\end{df}

\begin{prop}
Swapping two arguments (given they both belong to the same vector space)
will change the sign of an alternating function.
\end{prop}

\begin{proof}
It suffices to show that $f(a, b) = -f(b, a)$, or $f(a, b) + f(b, a) =
0$, but this is trivial. Why it reduces to a two variable function is
left as an exercise to the reader.
\end{proof}

\begin{rem}
If $f$ is not necessarily linear, we require both $f(a, a) = 0$ and
$f(a, b) + f(b, a) = 0$.
\end{rem}

\begin{thm}[Determinant Thing]
There exists a unique $n$-linear, alternating function $\det : V \times
\cdots \times V \rightarrow F$ isomorphic to $\det : \mathcal{M}(n, n)
\rightarrow F$ satisfying $\det(I) = 1$. This function is given by
$\det(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod A_{i \sigma(i)}$.
Basically what this is saying is that $\det$ should be $n$-linear in the
rows and columns of an $n \times n$ matrix.
\end{thm}

\begin{proof}
%TODO: finish
This nasty expansion for the determinant is credited to Leibniz. In this
proof, columns and rows can be freely interchanged to get the same
result.

\textbf{Existence}: We show that the Leibniz formula is $n$-linear and
alternating, as it obviously takes on $1$ at $I$. Let $r_i$ denote the
$i$-th row of a matrix $A$ and $r_i^j$ denote the $j$-th entry in the
$i$-th row. Then
\[ \begin{aligned}
\det(r_1, \dots, c r_j, \dots, r_n) &= \sum_{\sigma \in S_n}
\sgn(\sigma) cr_j^{\sigma(j)} \prod_{i = 1, i \neq j}^n r_i^{\sigma(i)}
\\
&= c \sum_{\sigma \in S_n} \sgn(\sigma) \prod r_i^{\sigma(i)} \\
&= c\det(r_1, \dots, r_n), \\
\det(r_1, \dots, v + r_j, \dots, r_n) &= \sum_{\sigma \in S_n}
\sgn(\sigma) \left(v^{\sigma(j)} + r_j^{\sigma(j)}\right) \prod_{i = 1,
i \neq j}^n r_i^{\sigma(i)} \\
&= \sum_{\sigma \in S_n} \sgn(\sigma) v^{\sigma(j)} \prod_{i = 1, i \neq
j}^n r_i^{\sigma(i)} + \sum_{\sigma \in S_n} \sgn(\sigma) \prod
r_i^{\sigma(i)} \\
&= \det(r_1, \dots, v, \dots, r_n) + \det(r_1, \dots, r_n),
\end{aligned} \]
so $\det$ is $n$-linear. Additionally, if $\tau_j^k$ is the
transposition that swaps the $j$-th and $k$-th entries and we let
$\sigma' = \tau_j^k \sigma$, we also have
\[ \begin{aligned}
\det(A) &= \sum_{\sigma \in S_n, \sigma(j) < \sigma(k)} \sgn(\sigma)
r_j^{\sigma(j)} r_k^{\sigma(k)} \prod_{i = 1, i \neq j, k}
r_i^{\sigma(i)} + \sgn(\sigma') r_j^{\sigma'(j)} r_k^{\sigma'(j)}
\prod_{i = 1, i \neq j, k} r_i^{\sigma'(j)} \\
&= \sum_{\sigma \in S_n, \sigma(j) < \sigma(k)} \sgn(\sigma)
\left(\prod_{i = 1, i \neq j, k}^n r_i^{\sigma(i)}\right)
\left(r_j^{\sigma(j)} r_k^{\sigma(k)} - r_j^{\sigma(k)}
r_k^{\sigma(j)}\right).
\end{aligned} \]
Hence if $r_j = r_k$, $\det(A) = 0$ so $\det$ is alternating as well. We
have now shown that the Leibniz formula does indeed give a $n$-linear,
alternating function on the row-space of a matrix.

\textbf{Uniqueness}: Let $f$ be another such function. We can represent
each row $r_i = \sum_k \mathcal{E}_k r_i^k$ (for columns, swap the
order), where $\mathcal{E}_k$ represents the $k$-th row of $I$. By
linearity, we then have
\[ f(A) = f\left(\sum_k \mathcal{E}_k r_1^k, \dots, \sum_k \mathcal{E}_k
r_n^k\right) = \sum_{k_1, \dots, k_n = 1}^n \left(\prod_{i = 1}^n
r_i^{k_i}\right) f(\mathcal{E}_{k_1}, \dots, \mathcal{E}_{k_n}). \]
However, $f$ is alternating, so it only suffices to consider
permutations, so we have
\[ f(A) = \sum_{\sigma \in S_n} \left(\prod_i r_i^{\sigma(i)}\right)
f(\mathcal{E}_{\sigma(1)}, \dots, \mathcal{E}_{\sigma(n)}). \]
We now use alternating again to rearrange the term on the right to
$f(I)$, and we use $\sgn$ to count the number of transpositions needed,
so we ultimately have
\[ f(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_i r_i^{\sigma(i)}
f(I). \]
Now use $f(I) = 1$ to get the desired result.
\end{proof}

\begin{rem}
Because the determinant is unique, you do not have to show that an
alternate form for the determinant takes on that ugly Leibniz formula.
You just have to show that it is $n$-linear, alternating, and $1$ on the
identity.
\end{rem}

\begin{prb}
Verify that the Leibniz formula produces the correct results for the $2
\times 2$ and $3 \times 3$ case.
\end{prb}

\begin{proof}[Solution]
Trivial.
\end{proof}
